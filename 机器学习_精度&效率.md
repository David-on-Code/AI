
NVIDIA 的研究人员发布了 MegatronLM。这是一个拥有 83 亿个参数（比 BERT 大 24 倍）的大型 transformer 模型，它在各种语言任务上都达到了最先进的性能。虽然这无疑是一项令人印象深刻的技术成就。  
### 深度学习是否朝着正确的方向发展？传统方法？两者的结合？新的方法？  
单是这些参数在磁盘上的权重就超过了 33GB，训练最终的模型需要 512v100 GPU 连续运行 9.2 天。考虑到每张卡的能量需求，训练这一模型所用的能量量是美国平均年能源消耗量的3倍多。  
有许多例子表明，大量的模型正在被训练，以便在各种基准上获得更高的精度。尽管 MegatronLM 比 BERT 大24倍，但它在语言建模方面只比 BERT 强34%。  
世界上每一个公共和私有云中可能只有不到 1 亿个处理器，但现在已经有 30 亿部手机、120 亿部物联网设备和 1500 亿个微控制器。从长远来看，正是这些小型、低功耗的设备最需要深度学习，大规模的模型根本不会是一个选择。  
### 小样本学习？？？？   
### 深度学习，高准确性or高效率or最大数量的人使用？  
### 技术驱动or需求驱动？  
